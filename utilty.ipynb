{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9f049b",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c8499",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed627f4",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.transform = transform\n",
    "        self.cifar10_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar10_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.cifar10_data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))[\"image\"]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, p=0.5),\n",
    "    A.CoarseDropout(max_holes=1, max_height=16, max_width=16, min_holes=1, min_height=16, min_width=16, fill_value=[0.5, 0.5, 0.5], mask_fill_value=None, p=0.5),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcaac29",
   "metadata": {},
   "source": [
    "## Data Vizualiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ea930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ec12b",
   "metadata": {},
   "source": [
    "## Dataloader Arguments & Test/Train Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beaceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"CUDA Available?\", cuda)\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# dataloader arguments - something you'll fetch these from cmdprmt\n",
    "dataloader_args = dict(shuffle=True, batch_size=512, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
    "\n",
    "# train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac0dcd",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  model.train()\n",
    "  pbar = tqdm(train_loader)\n",
    "  correct = 0\n",
    "  processed = 0\n",
    "  for batch_idx, (data, target) in enumerate(pbar):\n",
    "    # get samples\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Init\n",
    "    optimizer.zero_grad()\n",
    "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
    "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model(data)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.nll_loss(y_pred, target)\n",
    "    train_losses.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update pbar-tqdm\n",
    "\n",
    "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    processed += len(data)\n",
    "\n",
    "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "    train_acc.append(100*correct/processed)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d07176",
   "metadata": {},
   "source": [
    "## Plot Train and Test loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [t_items.item() for t_items in train_losses]\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
    "axs[0, 0].plot(t)\n",
    "axs[0, 0].set_title(\"Training Loss\")\n",
    "axs[1, 0].plot(train_acc)\n",
    "axs[1, 0].set_title(\"Training Accuracy\")\n",
    "axs[0, 1].plot(test_losses)\n",
    "axs[0, 1].set_title(\"Test Loss\")\n",
    "axs[1, 1].plot(test_acc)\n",
    "axs[1, 1].set_title(\"Test Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
